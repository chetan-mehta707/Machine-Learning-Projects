{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tables\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = '/Users/chetan/Documents/Git Projects/Machine Learning/Computer Vision Problem/Dataset/dataset.hdf5'  # address to where you want to save the hdf5 file\n",
    "subtract_mean = True\n",
    "batch_size = 50\n",
    "nb_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (259, 128, 128, 3)  train_label (259,)\n",
      "test_data: (87, 128, 128, 3)  test_label: (87,)\n",
      "val_data: (86, 128, 128, 3)  val_label: (86,)\n"
     ]
    }
   ],
   "source": [
    "hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "# subtract the training mean\n",
    "if subtract_mean:\n",
    "    mm = hdf5_file.root.train_mean[0]\n",
    "    mm = mm[np.newaxis, ...]\n",
    "\n",
    "# Total number of samples\n",
    "train_data = np.array(hdf5_file.root.train_img)\n",
    "train_label = np.array(hdf5_file.root.train_labels)\n",
    "\n",
    "test_data = np.array(hdf5_file.root.test_img)\n",
    "test_label = np.array(hdf5_file.root.test_labels)\n",
    "\n",
    "val_data = np.array(hdf5_file.root.val_img)\n",
    "val_label = np.array(hdf5_file.root.val_labels)\n",
    "\n",
    "print('train data:',train_data.shape,' train_label',train_label.shape)\n",
    "print('test_data:',test_data.shape,' test_label:',test_label.shape)\n",
    "print('val_data:',val_data.shape,' val_label:',val_label.shape)\n",
    "\n",
    "# create list of batches to shuffle the data\n",
    "#     batches_list = list(range(int(ceil(float(data_num) / batch_size))))\n",
    "#     shuffle(batches_list)\n",
    "\n",
    "#     # loop over batches\n",
    "#     for n, i in enumerate(batches_list):\n",
    "#         i_s = i * batch_size  # index of the first image in this batch\n",
    "#         i_e = min([(i + 1) * batch_size, data_num])  # index of the last image in this batch\n",
    "#         print('i_s:',i_s,' i_e:',i_e)\n",
    "\n",
    "#         # read batch images and remove training mean\n",
    "#         images = hdf5_file.root.train_img[i_s:i_e]\n",
    "#         print('len:',len(images))\n",
    "#         if subtract_mean:\n",
    "#             images -= mm\n",
    "\n",
    "#         # read labels and convert to one hot encoding\n",
    "#         labels = hdf5_file.root.train_labels[i_s:i_e]\n",
    "#         labels_one_hot = np.zeros((len(images), nb_class))\n",
    "#         labels_one_hot[np.arange(len(images)), labels] = 1\n",
    "\n",
    "#         print(n+1, '/', len(batches_list))\n",
    "\n",
    "#         print (labels[0], labels_one_hot[0, :])\n",
    "#         plt.imshow(images[0])\n",
    "#         plt.show()\n",
    "\n",
    "#         if n == 5:  # break after 5 batches\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/envs/Python35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Applications/Anaconda/anaconda3/envs/Python35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 2\n",
      "(259, 2) train samples\n",
      "(87, 2) test samples\n",
      "(86, 2) validation samples\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# one-hot encode the labels\n",
    "num_classes = len(np.unique(train_label))\n",
    "train_label = np_utils.to_categorical(train_label, num_classes)\n",
    "test_label = np_utils.to_categorical(test_label, num_classes)\n",
    "val_label = np_utils.to_categorical(val_label, num_classes)\n",
    "\n",
    "# print shape of training set\n",
    "print('num_classes:', num_classes)\n",
    "\n",
    "# print number of training, validation, and test images\n",
    "print(train_label.shape, 'train samples')\n",
    "print(test_label.shape, 'test samples')\n",
    "print(val_label.shape, 'validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               8192500   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 8,204,046\n",
      "Trainable params: 8,204,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='tanh'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='tanh'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coming here\n",
      "Train on 259 samples, validate on 86 samples\n",
      "Epoch 1/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 6s 24ms/step - loss: 5.4186 - acc: 0.5946 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.24775, saving model to model.weights.best.hdf5\n",
      "Epoch 2/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 17ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "batch size here: [(0, 32), (32, 64), (64, 96), (96, 128), (128, 160), (160, 192), (192, 224), (224, 256), (256, 259)]\n",
      "259/259 [==============================] - 4s 16ms/step - loss: 6.0365 - acc: 0.6255 - val_loss: 5.2478 - val_acc: 0.6744\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "hist = model.fit(train_data, train_label, batch_size=None, epochs=20,\n",
    "          validation_data=(val_data, val_label),callbacks=[checkpointer], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('model.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.724137934460037\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_data, test_label, verbose=0)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
