{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[2, .75],[.75, 2]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "x3 = np.random.multivariate_normal([2, 8], [[0, .75],[.75, 0]], num_observations)\n",
    "\n",
    "simulated_separableish_features = np.vstack((x1, x2, x3)).astype(np.float32)\n",
    "print(\"simulated_separableish_features\",simulated_separableish_features)\n",
    "simulated_labels = np.hstack((np.zeros(num_observations),\n",
    "\t\t\t\tnp.ones(num_observations), np.ones(num_observations) + 1))\n",
    "print(\"simulated_labels\",simulated_labels)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = simulated_labels, alpha = .4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_onehot = np.zeros((simulated_labels.shape[0], 3)).astype(int)\n",
    "labels_onehot[np.arange(len(simulated_labels)), simulated_labels.astype(int)] = 1\n",
    "\n",
    "train_dataset, test_dataset, \\\n",
    "train_labels, test_labels = train_test_split(\n",
    "    simulated_separableish_features, labels_onehot, test_size = .1, random_state = 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_activation(data_array):\n",
    "    return np.maximum(data_array, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(output_array):\n",
    "    logits_exp = np.exp(output_array)\n",
    "    return logits_exp / np.sum(logits_exp, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_softmax_loss_array(softmax_probs_array, y_onehot):\n",
    "    indices = np.argmax(y_onehot, axis = 1).astype(int)\n",
    "    predicted_probability = softmax_probs_array[np.arange(len(softmax_probs_array)), indices]\n",
    "    log_preds = np.log(predicted_probability)\n",
    "    loss = -1.0 * np.sum(log_preds) / len(log_preds)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regularization_L2_softmax_loss(reg_lambda, weight1, weight2):\n",
    "    weight1_loss = 0.5 * reg_lambda * np.sum(weight1 * weight1)\n",
    "    weight2_loss = 0.5 * reg_lambda * np.sum(weight2 * weight2)\n",
    "    return weight1_loss + weight2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = train_dataset\n",
    "training_labels = train_labels\n",
    "\n",
    "hidden_nodes = 5\n",
    "num_labels = training_labels.shape[1]\n",
    "num_features = training_data.shape[1]\n",
    "learning_rate = .01\n",
    "reg_lambda = .01\n",
    "\n",
    "# Weights and Bias Arrays, just like in Tensorflow\n",
    "layer1_weights_array = np.random.normal(0, 1, [num_features, hidden_nodes])  #2*5\n",
    "layer2_weights_array = np.random.normal(0, 1, [hidden_nodes, num_labels])    #5*3\n",
    "\n",
    "layer1_biases_array = np.zeros((1, hidden_nodes))  #1*5\n",
    "layer2_biases_array = np.zeros((1, num_labels))    #1*3\n",
    "\n",
    "\n",
    "for step in range(5001):\n",
    "\n",
    "    input_layer = np.dot(training_data, layer1_weights_array)\n",
    "    hidden_layer = relu_activation(input_layer + layer1_biases_array)\n",
    "    output_layer = np.dot(hidden_layer, layer2_weights_array) + layer2_biases_array\n",
    "    output_probs = softmax(output_layer)\n",
    "    \n",
    "    loss = cross_entropy_softmax_loss_array(output_probs, training_labels)\n",
    "    loss += regularization_L2_softmax_loss(reg_lambda, layer1_weights_array, layer2_weights_array)\n",
    "\n",
    "    output_error_signal = (output_probs - training_labels) / output_probs.shape[0]\n",
    "    \n",
    "    error_signal_hidden = np.dot(output_error_signal, layer2_weights_array.T) \n",
    "    error_signal_hidden[hidden_layer <= 0] = 0\n",
    "    \n",
    "    gradient_layer2_weights = np.dot(hidden_layer.T, output_error_signal)\n",
    "    gradient_layer2_bias = np.sum(output_error_signal, axis = 0, keepdims = True)\n",
    "    \n",
    "    gradient_layer1_weights = np.dot(training_data.T, error_signal_hidden)\n",
    "    gradient_layer1_bias = np.sum(error_signal_hidden, axis = 0, keepdims = True)\n",
    "\n",
    "    gradient_layer2_weights += reg_lambda * layer2_weights_array\n",
    "    gradient_layer1_weights += reg_lambda * layer1_weights_array\n",
    "\n",
    "    layer1_weights_array -= learning_rate * gradient_layer1_weights\n",
    "    layer1_biases_array -= learning_rate * gradient_layer1_bias\n",
    "    layer2_weights_array -= learning_rate * gradient_layer2_weights\n",
    "    layer2_biases_array -= learning_rate * gradient_layer2_bias\n",
    "    \n",
    "    if step % 250 == 0:\n",
    "            print('Loss at step {0}: {1}'.format(step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    preds_correct_boolean =  np.argmax(predictions, 1) == np.argmax(labels, 1)\n",
    "    correct_predictions = np.sum(preds_correct_boolean)\n",
    "    accuracy = 100.0 * correct_predictions / predictions.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = np.dot(test_dataset, layer1_weights_array)\n",
    "hidden_layer = relu_activation(input_layer + layer1_biases_array)\n",
    "scores = np.dot(hidden_layer, layer2_weights_array) + layer2_biases_array\n",
    "probs = softmax(scores)\n",
    "print('Test accuracy: {0}%'.format(accuracy(probs, test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_flat = np.argmax(test_labels, axis = 1)\n",
    "predictions = np.argmax(probs, axis = 1)\n",
    "print(predictions)\n",
    "c=predictions == labels_flat\n",
    "print(np.sum(c))\n",
    "print(predictions.shape[0]-np.count_nonzero(c))\n",
    "print(c.shape)\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.scatter(test_dataset[:, 0], test_dataset[:, 1], c = predictions == labels_flat, alpha = .8, s = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
